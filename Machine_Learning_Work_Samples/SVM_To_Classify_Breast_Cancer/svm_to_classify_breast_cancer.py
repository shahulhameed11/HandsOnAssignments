# -*- coding: utf-8 -*-
"""SVM to Classify Breast Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fhQyTIT2CDoTQUjGuraewmsmQaiJNKQ8
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import SelectKBest, SelectPercentile

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""## Load breast cancer dataset"""

from sklearn.datasets import load_breast_cancer

cancer=load_breast_cancer()

df_cancer=pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns=np.append(cancer['feature_names'],['target']))

df_cancer.head()

X=df_cancer.drop(['target'], axis=1)

y=df_cancer['target']

X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2, random_state=5)
X_train.shape, y_train.shape

"""## Feature Selection using Mutual correlation, this measures the value between each independent variable with respect to dependent variable, higher the value means high dependancy lower the value means low dependency wrt target variable"""

mi=mutual_info_classif(X_train, y_train)
mi=pd.Series(mi)
mi.index=X_train.columns
mi.sort_values(ascending=False)

mi.sort_values(ascending=False).plot.bar()

"""## It is clear that last few columns are having low dependency wrt target variable so dropping last two columns. Selecting top 28 features"""

# Here we are dropping the last two columns, there is no criteria  to drop how many columns but here with top 26 and 28 features we are able to obtain the same accuracy
sel_=SelectKBest(mutual_info_classif, k=28).fit(X_train, y_train)
X_train.columns[sel_.get_support()]

X_train=X_train[['mean radius', 'mean texture', 'mean perimeter', 'mean area',
       'mean smoothness', 'mean compactness', 'mean concavity',
       'mean concave points', 'mean symmetry', 'mean fractal dimension',
       'radius error', 'perimeter error', 'area error', 'smoothness error',
       'compactness error', 'concavity error', 'concave points error',
       'fractal dimension error', 'worst radius', 'worst texture',
       'worst perimeter', 'worst area', 'worst smoothness',
       'worst compactness', 'worst concavity', 'worst concave points',
       'worst symmetry', 'worst fractal dimension']]

X_test=X_test[['mean radius', 'mean texture', 'mean perimeter', 'mean area',
       'mean smoothness', 'mean compactness', 'mean concavity',
       'mean concave points', 'mean symmetry', 'mean fractal dimension',
       'radius error', 'perimeter error', 'area error', 'smoothness error',
       'compactness error', 'concavity error', 'concave points error',
       'fractal dimension error', 'worst radius', 'worst texture',
       'worst perimeter', 'worst area', 'worst smoothness',
       'worst compactness', 'worst concavity', 'worst concave points',
       'worst symmetry', 'worst fractal dimension']]

"""## Normalizing training and testing set"""

min_train=X_train.min()
range_train=(X_train-min_train).max()
X_train_scaled=(X_train-min_train)/range_train

min_test=X_test.min()
range_test=(X_test-min_test).max()
X_test_scaled=(X_test-min_test)/range_test

"""## Applying SVM to the training set"""

svc_model=SVC()

svc_model.fit(X_train_scaled, y_train)

"""## Predicting the model"""

y_predict=svc_model.predict(X_test_scaled)

cm=confusion_matrix(y_test, y_predict)

sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict))

"""## Accuracy 97%

## Tuning the hyper parameters of SVM by training the model with a set of gamma and c values using GridSearchCV which identifies the best parameters of SVM
"""

param_grid={'C':[0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel':['rbf']}
from sklearn.model_selection import GridSearchCV
grid=GridSearchCV(SVC(), param_grid, refit=True, verbose=4)

grid.fit(X_train_scaled, y_train)

"""## GridSearchCV results out the best params"""

grid.best_params_

grid_predict=grid.predict(X_test_scaled)

cm=confusion_matrix(y_test, grid_predict)

sns.heatmap(cm, annot=True)

print(classification_report(y_test, grid_predict))

